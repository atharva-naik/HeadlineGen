{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning BART for abstractive text summarisation with fastai2\n",
    "\n",
    "A great thing about working in NLP at the moment is being able to park a hard problem for a few weeks and discovering the community making massive amounts of progress on your behalf. I used to be overwhelmed by the challenge of just training a summarisation model to generate plausible looking text without burning through tonnes of cash on GPUs. Then [BertExtAbs](../finetuning-bertsumextabs) came along and solved that problem. Unfortunately, it still gernerated incoherent sentences sometimes and had a habit of confusing entities in an article. You certainly couldn't trust it to convey the facts of an article reliably.\n",
    "\n",
    "Enter BART (Bidirectional and Auto-Regressive Transformers). Here we have a model that generates staggeringly good summaries and has a wonderful implementation from Sam Shleifer at HuggingFace. It's still a work in progress, but after digging around in the Transformers pull requests and with help from [Morgan McGuire's FastHugs notebook](https://github.com/morganmcg1/fasthugs) I have put together this notebook for fine-tuning BART and generating summaries. Feedback welcome!\n",
    "\n",
    "I should mention that this a big model requiring big inputs. For fine-tuning I've been able to get a batch size of 4 and a maximum sequence length of 512 on an AWS P3.2xlarge (~Â£4 an hour).\n",
    "\n",
    "We begin with a bunch of imports and an args object for storing variables we will need. We'll be finetuning the model on the Curation Corpus of abstractive text summaries. We load it into a dataframe using Pandas. For more information about how to access this dataset for your own purposes please see our [article introducing the dataset](https://medium.com/curation-corporation/teaching-an-ai-to-abstract-a-new-dataset-for-abstractive-auto-summarisation-5227f546caa8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import logging\n",
    "logging.getLogger().setLevel(100)\n",
    "from fastprogress import progress_bar\n",
    "from fastai2.basics import Transform, Datasets, RandomSplitter, Module, Learner, ranger, params, load_learner\n",
    "from fastai2.text.all import TensorText\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import PreTrainedTokenizer, BartTokenizer, BartForConditionalGeneration, BartConfig \n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully we will be able to increase our batch size and/or maximum sequence lengths when some pull requests to reduce the model's memory footprint get merged into the Transformers repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "args = Namespace(\n",
    "    batch_size=4,\n",
    "    max_seq_len=512,\n",
    "    data_path=\"../data/private_dataset.file\",\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), # ('cpu'),\n",
    "    stories_folder='../data/my_own_stories',\n",
    "    subset=None,\n",
    "    test_pct=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_feather(args.data_path).iloc[:args.subset]\n",
    "ds = ds[ds['summary'] != '']\n",
    "train_ds, test_ds = train_test_split(ds, test_size=args.test_pct, random_state=42)\n",
    "valid_ds, test_ds = train_test_split(test_ds, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass our data to the model in our fastai2 learner object we need a dataloader. To create a dataloader we need a Datasets object, batch size, and device type. To create a Datasets object, we have to pass a few things:\n",
    "- Our raw data which in our case is a Pandas dataframe\n",
    "- A list of transforms. Or to be more precise a list containing the list of transforms to perform on our inputs and a list of transforms to perform on our desired outputs. I've defined a transform below that encodes the text using the BART tokenizer. Mostly it will be the encodes class method that gets called by fastai2. However the decodes method can also be useful if you want to reverse the process.\n",
    "- We will also split our data into training and validation datasets here, using fastai2's RandomSplitter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('bart-large-cnn', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm still exploring whether it is necessary to pass any of the masks and other ids manually or if it is handled for us. Any advice here would be much appreciated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform(Transform):\n",
    "    def __init__(self, tokenizer:PreTrainedTokenizer, column:str):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.column = column\n",
    "        \n",
    "    def encodes(self, inp):  \n",
    "        tokenized = self.tokenizer.batch_encode_plus(\n",
    "            [list(inp[self.column])],\n",
    "            max_length=args.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return TensorText(tokenized['input_ids']).squeeze()\n",
    "        \n",
    "    def decodes(self, encoded):\n",
    "        decoded = [\n",
    "            self.tokenizer.decode(\n",
    "                o, \n",
    "                skip_special_tokens=True, \n",
    "                clean_up_tokenization_spaces=False\n",
    "            ) for o in encoded\n",
    "        ]\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms = [DataTransform(tokenizer, column='text')]\n",
    "y_tfms = [DataTransform(tokenizer, column='summary')]\n",
    "dss = Datasets(\n",
    "    train_ds, \n",
    "    tfms=[x_tfms, y_tfms], \n",
    "    splits=RandomSplitter(valid_pct=0.1)(range(train_ds.shape[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dss.dataloaders(bs=args.batch_size, device=args.device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function lets us choose between loading the model architecture with Facebook's pretrained weights, the model architecture with our own weights stored locally, or the model architecture with no pretraining at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_model(config, pretrained=False, path=None): \n",
    "    if pretrained:    \n",
    "        if path:\n",
    "            model = BartForConditionalGeneration.from_pretrained(\n",
    "                \"bart-large-cnn\", \n",
    "                state_dict=torch.load(path, map_location=torch.device(args.device)), \n",
    "                config=config\n",
    "            )\n",
    "        else: \n",
    "            model = BartForConditionalGeneration.from_pretrained(\"bart-large-cnn\", config=config)\n",
    "    else:\n",
    "        model = BartForConditionalGeneration()\n",
    "\n",
    "    return model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will return a lot of different things, but we only want the weights to calculate the loss when training, so we will wrap the model in this class to control what gets passed to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastaiWrapper(Module):\n",
    "    def __init__(self):\n",
    "        self.config = BartConfig(vocab_size=50264, output_past=True)\n",
    "        self.bart = load_hf_model(config=self.config, pretrained=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.bart(x)[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of seq2seq tasks as a series of attempts to categorise which word should come next. Cross entropy loss is a pretty good loss function for this use case. We want to normalise it by how many non padding words are in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarisationLoss(Module):\n",
    "    def __init__(self):\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        x = F.log_softmax(output, dim=-1)\n",
    "        norm = (target != 1).data.sum()\n",
    "        return self.criterion(x.contiguous().view(-1, x.size(-1)), target.contiguous().view(-1)) / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fine-tuning the model we start by just training the top layer(s). You can experiment by unfreezing layers further down in the decoder, and then (if you're feeling bold) then encoder. fastai2 provides an easy way to split the model up into groups with frozen or unfrozen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_splitter(model):\n",
    "    return [\n",
    "        params(model.bart.model.encoder), \n",
    "        params(model.bart.model.decoder.embed_tokens),\n",
    "        params(model.bart.model.decoder.embed_positions),\n",
    "        params(model.bart.model.decoder.layers),\n",
    "        params(model.bart.model.decoder.layernorm_embedding),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been experimenting with half precision training. In theory this will save a lot of memory. However, I find my loss quickly becomes a bunch of nans. This may be an issue with HuggingFace's implementation or it may be an issue with my code. I'll update if I work out how to get fp16() working. Do let me know if you have any ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls, \n",
    "    FastaiWrapper(), \n",
    "    loss_func=SummarisationLoss(), \n",
    "    opt_func=ranger,\n",
    "    splitter=bart_splitter\n",
    ")#.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been finding that the learning rate finder suggests values that are too high. Your mileage may vary though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_flat_cos(\n",
    "    1,\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do carry on unfreezing layers, you may find that you need to reduce your batch size to fit everything in memory. Also you should probably lower your learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.dls.train.bs = args.batch_size//2\n",
    "learn.dls.valid.bs = args.batch_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_flat_cos(\n",
    "    2,\n",
    "    lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is done we can export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('../models/fintuned_bart.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner('../models/fintuned_bart.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code for generating the summaries comes from [Sam Shleifer's example in the Transformers repository](https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/evaluate_cnn.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def generate_summaries(lns, out_file, batch_size=4):\n",
    "    dec = []\n",
    "    for batch in progress_bar(list(chunks(lns, batch_size))):\n",
    "        dct = tokenizer.batch_encode_plus(\n",
    "            batch, \n",
    "            max_length=1024, \n",
    "            return_tensors=\"pt\", \n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "        \n",
    "        summaries = learn.model.bart.to(args.device).generate(\n",
    "            input_ids=dct[\"input_ids\"].to(args.device),\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            max_length=142,\n",
    "            min_length=56,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "        \n",
    "        dec.extend([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries])\n",
    "        \n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns = [\" \" + x.rstrip() for x in list(test_ds['text'])[:8]]\n",
    "bart_sums = generate_summaries(lns, f'{args.stories_folder}/output.txt', batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in bart_sums[:8]:\n",
    "    print(s)\n",
    "    print(\"***************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py374",
   "language": "python",
   "name": "py374"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
